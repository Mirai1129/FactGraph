"""
PTS (å…¬è¦–) æ–‡ç« çˆ¬èŸ²
===================

çˆ¬å–åˆ†é¡é  (/category/1) å‰ `MAX_PAGES` é çš„æ‰€æœ‰æ–‡ç« ã€‚
è¼¸å‡ºä¸€æ”¯ JSON æª”ï¼š
    FactGraph/data/raw/news/pts_<timestamp>.json
    timestamp = yyyymmddHHMM (Asia/Taipei)

æ¯å‰‡æ–‡ç« å­—æ®µï¼š
    date, publisher, category, title, content, label
"""

from __future__ import annotations

import json
import re
import time
from datetime import datetime
from hashlib import md5
from pathlib import Path
from typing import List, Optional
from zoneinfo import ZoneInfo

import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åƒæ•¸ï¼å¸¸æ•¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORD: str = "äº¬è¯åŸ"  # å¦‚éœ€é—œéµå­—éæ¿¾ï¼Œå¯è‡ªè¡Œä½¿ç”¨
BASE_URL: str = "https://news.pts.org.tw"
SEARCH_URL: str = f"{BASE_URL}/category/1?page={{}}"

HEADERS: dict[str, str] = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9",
    "Connection": "keep-alive",
}

MAX_PAGES: int = 1
TPE_TZ = ZoneInfo("Asia/Taipei")
OUTPUT_DIR = Path("FactGraph/data/raw/news/pts/")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å·¥å…·å‡½å¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_CLEAN_PAT = re.compile(
    r'[\n"ã€Œã€ï¼šï¼›:;,{}\[\]]|\s{2,}',
    flags=re.UNICODE,
)


def clean_text(text: str) -> str:
    """ç§»é™¤å¤šé¤˜ç¬¦è™Ÿèˆ‡ç©ºç™½ã€‚"""
    return _CLEAN_PAT.sub(" ", text).strip()


def parse_date(date_text: str) -> str:
    """å°‡å„å¼æ—¥æœŸå­—ä¸²è½‰æˆ YYYY-MM-DDã€‚å¤±æ•—å‰‡å›å‚³ç©ºå­—ä¸²ã€‚"""
    date_text = clean_text(date_text)
    date_formats = (
        "%Y/%m/%d",
        "%Y-%m-%d",
        "%Yå¹´%mæœˆ%dæ—¥",
        "%Y.%m.%d",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d %H:%M",
    )
    for fmt in date_formats:
        try:
            # å»æ‰æ™‚é–“ï¼Œåƒ…ä¿ç•™æ—¥æœŸéƒ¨åˆ†
            cleaned = date_text.split(" ")[0]
            dt_obj = datetime.strptime(cleaned, fmt)
            return dt_obj.strftime("%Y-%m-%d")
        except ValueError:
            continue
    return ""


def make_article_id(url: str) -> str:
    """ç”Ÿæˆç°¡çŸ­ä¸”å”¯ä¸€çš„æ–‡ç«  IDï¼ˆ8 ä½ md5ï¼‰ã€‚"""
    return f"PTS_{md5(url.encode()).hexdigest()[:8]}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ–‡ç« è§£æ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_html(url: str, session: requests.Session) -> Optional[str]:
    """GET è«‹æ±‚ï¼ŒæˆåŠŸå›å‚³ HTMLï¼›å¤±æ•—å›å‚³ Noneã€‚"""
    try:
        res = session.get(url, headers=HEADERS, timeout=10)
        if res.status_code == 200:
            res.encoding = "utf-8"
            return res.text
        print(f"âŒ å¤±æ•— {res.status_code} â†’ {url}")
    except requests.RequestException as exc:
        print(f"âŒ è«‹æ±‚éŒ¯èª¤ï¼š{exc} â†’ {url}")
    return None


def parse_article(url: str, session: requests.Session) -> Optional[dict]:
    """æ“·å–å–®ç¯‡æ–°èè³‡æ–™ã€‚å¤±æ•—å›å‚³ Noneã€‚"""
    html = fetch_html(url, session)
    if not html:
        return None

    soup = BeautifulSoup(html, "html.parser")

    # æ¨™é¡Œ
    title_tag = soup.select_one("h1.article-title") or soup.select_one("h1")
    title = clean_text(title_tag.text) if title_tag else ""
    if not title:
        print("âš ï¸ ç„¡æ¨™é¡Œï¼Œè·³é")
        return None

    # æ—¥æœŸ
    date_tag = soup.select_one("time") or soup.select_one("span.date")
    date_raw = date_tag.text if date_tag else ""
    date_str = parse_date(date_raw)

    # åˆ†é¡
    category = "æœªçŸ¥åˆ†é¡"
    info_div = soup.select_one("div.news-info")
    if info_div:
        all_txt = clean_text(info_div.get_text())
        if "|" in all_txt:
            category = all_txt.split("|")[1].strip()
        else:
            category = all_txt or category

    # å…§å®¹
    content_div = soup.select_one("div.post-article.text-align-left")
    content_raw = clean_text(content_div.get_text()) if content_div else ""
    sentences = [s for s in content_raw.split("ã€‚") if s.strip()]
    content = "\n".join(f"{s}ã€‚" for s in sentences)

    return {
        "date": date_str,
        "publisher": "pts",
        "category": category,
        "title": title,
        "content": content,
        "label": True,
        "url": url,  # è‹¥ä¸éœ€å­˜ URLï¼Œå¯åˆªé™¤æ­¤è¡Œ
        "id": make_article_id(url),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æœå°‹é è§£æ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_links(html: str) -> List[str]:
    """å¾åˆ—è¡¨é æ‹¿å‡ºæ‰€æœ‰æ–‡ç« é€£çµã€‚"""
    soup = BeautifulSoup(html, "html.parser")
    anchors = soup.select('h2 a[href*="/article/"]')
    links: list[str] = []
    for a in anchors:
        href = a.get("href", "")
        if href.startswith("/article/"):
            links.append(BASE_URL + href)
        elif href.startswith("https://news.pts.org.tw/article/"):
            links.append(href)
    return links


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»æµç¨‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_pts(max_pages: int = MAX_PAGES) -> None:
    """PTS çˆ¬èŸ²å…¥å£ã€‚"""
    start = datetime.now(TPE_TZ)
    print("é–‹å§‹æ™‚é–“:", start.strftime("%Y-%m-%d %H:%M:%S"))

    session = requests.Session()
    results: list[dict] = []
    seen_ids: set[str] = set()

    for page in range(1, max_pages + 1):
        print(f"\nğŸ” è§£æç¬¬ {page} é ")
        list_html = fetch_html(SEARCH_URL.format(page), session)
        if not list_html:
            break

        links = extract_links(list_html)
        print(f"âœ… æ‰¾åˆ° {len(links)} å‰‡é€£çµ")

        for idx, article_url in enumerate(links, 1):
            print(f"  â¡ï¸ ({idx}/{len(links)}) {article_url}")
            item = parse_article(article_url, session)
            if not item:
                continue
            if item["id"] in seen_ids:
                print("   â†ªï¸ é‡è¤‡ï¼Œè·³é")
                continue
            # è‹¥éœ€é—œéµå­—éæ¿¾ï¼Œå¯åœ¨æ­¤åˆ¤æ–·
            # if KEYWORD not in item["content"]:
            #     continue
            results.append(item)
            seen_ids.add(item["id"])
            time.sleep(2)  # ç¦®è²Œå»¶é²

    # â”€â”€ è¼¸å‡º JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    ts = start.strftime("%Y%m%d%H%M")
    out_path = OUTPUT_DIR / f"pts_{ts}.json"

    with out_path.open("w", encoding="utf-8") as fp:
        json.dump(results, fp, ensure_ascii=False, indent=2)

    end = datetime.now(TPE_TZ)
    print("\nğŸ‰ å®Œæˆï¼")
    print("è¼¸å‡ºæª”æ¡ˆï¼š", out_path.resolve())
    print(f"å…± {len(results)} ç­†æ–‡ç« ")
    print("çµæŸæ™‚é–“:", end.strftime("%Y-%m-%d %H:%M:%S"))
    print(f"è€—æ™‚ {(end - start).total_seconds():.1f} ç§’")


if __name__ == "__main__":
    scrape_pts()
