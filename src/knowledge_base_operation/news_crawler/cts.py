"""
CTS (è¯è¦–) Topic Crawler
=======================

æŠ“å–åˆ†é¡ï¼ˆæ”¿æ²»ã€åœ‹éš›ã€ç¤¾æœƒï¼‰æœ€æ–°æ–°èï¼Œè¼¸å‡º JSONï¼š

[
    {
        "date": "YYYY-MM-DD",
        "publisher": "CTS",
        "category": "æ”¿æ²»",
        "title": "æ¨™é¡Œ",
        "content": "å…§æ–‡â€¦",
        "label": true
    },
    ...
]

è¼¸å‡ºè·¯å¾‘ï¼š
    FactGraph/data/raw/news/cts_<timestamp>.json
    timestamp = yyyymmddHHMM (Asia/Taipei)
"""

from __future__ import annotations

import json
import random
import time
from datetime import datetime
from pathlib import Path
from zoneinfo import ZoneInfo

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¸¸æ•¸ï¼è¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TPE_TZ = ZoneInfo("Asia/Taipei")

BASE_URL = "https://news.cts.com.tw/"
CATEGORIES = {
    "politics": "æ”¿æ²»",
    "international": "åœ‹éš›",
    "society": "ç¤¾æœƒ",
}
SCROLL_COUNT: int = 3  # æ¯ä¸€åˆ†é¡å¾€ä¸‹æ»¾å‹•æ¬¡æ•¸
WAIT_BETWEEN_SCROLL: float = 3  # æ¯æ¬¡æ»¾å‹•å¾Œç­‰å¾…ç§’æ•¸
RANDOM_WAIT_MIN: float = 1.5  # é€²å…§æ–‡éš¨æ©Ÿç­‰å¾…ä¸‹é™
RANDOM_WAIT_MAX: float = 5.0  # é€²å…§æ–‡éš¨æ©Ÿç­‰å¾…ä¸Šé™
USER_AGENT: str = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/124.0.0.0 Safari/537.36"
)

OUTPUT_DIR = Path("FactGraph/data/raw/news/cts/")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Selenium Driver
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_driver(headless: bool = True) -> webdriver.Chrome:
    """Return a configured Chrome driver."""
    chrome_opts = Options()
    if headless:
        chrome_opts.add_argument("--headless=new")
    chrome_opts.add_argument("--disable-gpu")
    chrome_opts.add_argument("--no-sandbox")
    chrome_opts.add_argument("--start-maximized")
    chrome_opts.add_argument("--disable-dev-shm-usage")
    chrome_opts.add_argument(f"user-agent={USER_AGENT}")
    chrome_opts.add_experimental_option(
        "excludeSwitches", ["enable-automation"]
    )
    chrome_opts.add_experimental_option("useAutomationExtension", False)

    drv = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=chrome_opts,
    )
    # éš±è— webdriver ç‰¹å¾µ
    drv.execute_cdp_cmd(
        "Page.addScriptToEvaluateOnNewDocument",
        {
            "source": (
                "Object.defineProperty("
                "navigator,'webdriver',{get:()=>undefined});"
            )
        },
    )
    return drv


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å·¥å…·å‡½å¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scroll_page(driver: webdriver.Chrome, times: int = 1) -> None:
    """Scroll to the bottom `times` times to load dynamic content."""
    for i in range(times):
        driver.execute_script(
            "window.scrollTo(0, document.body.scrollHeight);"
        )
        print(f"  â†“ ç¬¬ {i + 1}/{times} æ¬¡æ»¾å‹•")
        time.sleep(WAIT_BETWEEN_SCROLL)


def parse_article(driver: webdriver.Chrome, url: str, category_tw: str) -> dict:
    """Open `url` and return structured article data."""
    driver.get(url)
    time.sleep(random.uniform(RANDOM_WAIT_MIN, RANDOM_WAIT_MAX))

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # æ¨™é¡Œ
    title_tag = soup.select_one("div.artical-titlebar h1")
    if not title_tag:
        raise ValueError("ç„¡æ³•æ“·å–æ¨™é¡Œ")
    title = title_tag.text.replace(" ", "").replace("ã€€", "")  # å»é™¤å…¨åŠå½¢ç©ºæ ¼

    # æ™‚é–“ â†’ YYYY-MM-DD
    time_tag = soup.select_one("div.titlebar-top time")
    raw_dt = datetime.strptime(time_tag.text.strip(), "%Y/%m/%d %H:%M")
    date_str = raw_dt.strftime("%Y-%m-%d")

    # å…§æ–‡
    paragraphs = soup.select("div.artical-content p")
    content_parts = [
        p.get_text(strip=True)
        for p in paragraphs
        if "å ±å°" not in p.text and "æ–°èä¾†æº" not in p.text
    ]
    for sym in ['\n', '"', 'ã€Œ', 'ã€', 'ï¼š', 'ï¼›', ':', ';', ',', '{', '}', '[', ']']:
        content_parts = [txt.replace(sym, "") for txt in content_parts]

    raw_content = "".join(content_parts)
    sentences = [s for s in raw_content.split("ã€‚") if s.strip()]
    content = "\n".join(f"{s}ã€‚" for s in sentences)

    return {
        "date": date_str,
        "publisher": "CTS",
        "category": category_tw,
        "title": title,
        "content": content,
        "label": True,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»æµç¨‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    """Crawl all categories and export to a timestamped JSON."""
    start_time = datetime.now(TPE_TZ)
    print("é–‹å§‹æ™‚é–“:", start_time.strftime("%Y-%m-%d %H:%M:%S"))

    driver = build_driver(headless=True)
    articles: list[dict] = []
    seen_titles: set[str] = set()  # ç•¶æ¬¡åŸ·è¡Œå…§å»é‡

    try:
        for slug, category_tw in CATEGORIES.items():
            url = f"{BASE_URL}{slug}/index.html"
            print(f"\nğŸ“„ è™•ç†åˆ†é¡é ï¼š{url}")
            driver.get(url)
            time.sleep(5)  # ç­‰ä¸»è¦å…§å®¹è¼‰å…¥

            print("ğŸ”„ æ»¾å‹•é é¢è¼‰å…¥æ›´å¤šå…§å®¹â€¦")
            scroll_page(driver, SCROLL_COUNT)

            soup = BeautifulSoup(driver.page_source, "html.parser")
            link_container = soup.select_one(
                "div.newslist-container.flexbox.one_row_style"
            )
            if not link_container:
                print("âš ï¸ æ‰¾ä¸åˆ°æ–°èæ¸…å–®å®¹å™¨ï¼Œè·³éæ­¤åˆ†é¡")
                continue

            news_links = [a["href"] for a in link_container.select("a")]
            print(f"ğŸ“‘ å…± {len(news_links)} ç­†é€£çµ")

            for idx, article_url in enumerate(news_links, 1):
                print(f"  â¡ï¸ ({idx}/{len(news_links)}) {article_url}")
                try:
                    article = parse_article(driver, article_url, category_tw)
                    if article["title"] in seen_titles:
                        print("   â†ªï¸ é‡è¤‡æ¨™é¡Œï¼Œè·³é")
                        continue
                    articles.append(article)
                    seen_titles.add(article["title"])
                except Exception as exc:  # pylint: disable=broad-except
                    print(f"   âš ï¸ è§£æå¤±æ•—ï¼š{exc}")
                finally:
                    driver.delete_all_cookies()
                    driver.get("about:blank")
                    time.sleep(random.uniform(RANDOM_WAIT_MIN, RANDOM_WAIT_MAX))

    finally:
        driver.quit()

    # è¼¸å‡º
    ts = start_time.strftime("%Y%m%d%H%M")
    output_path = OUTPUT_DIR / f"cts_{ts}.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as fp:
        json.dump(articles, fp, ensure_ascii=False, indent=2)

    end_time = datetime.now(TPE_TZ)
    elapsed = (end_time - start_time).total_seconds()
    print("\nâœ… å®Œæˆï¼")
    print("è¼¸å‡ºæª”æ¡ˆï¼š", output_path.resolve())
    print("çµæŸæ™‚é–“:", end_time.strftime("%Y-%m-%d %H:%M:%S"))
    print(f"ç¨‹å¼è€—æ™‚: {elapsed:.1f} ç§’")


if __name__ == "__main__":
    main()
