#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
answerer â”€ å•ç­”ä¸»æµç¨‹ï¼ˆOrchestratorï¼‰

è·è²¬åªåšã€Œæµç¨‹å”èª¿ã€ï¼Œä¸è™•ç†ç¹é›œæ¥­å‹™é‚è¼¯ï¼š
0. python -m src.qa.answerer.pipeline <id.txt>
1. è®€å–ä½¿ç”¨è€…å•é¡Œï¼ˆæª”æ¡ˆæˆ– stdinï¼‰ä¸¦å–å¾— slug
2. å‘¼å« GPT æŠ½å–ä¸‰å…ƒçµ„
3. ä»¥å‘é‡æœå°‹ KG ç›¸é—œæ•˜è¿°
4. å»é‡ï¼ˆç›¸ä¼¼åƒ…ä¿ç•™æœ€é•·æ¢ç›®ï¼‰
5. å‘¼å« GPT è©•ä¼°æœ€çµ‚çµæœ
6. ä¾è¼¸å…¥æª”åå‹•æ…‹è¼¸å‡º user_kg_*.txt èˆ‡ user_qa_judge_*.txt
"""

from __future__ import annotations

import argparse
import os
import re
import sys
from pathlib import Path

from .core.embedding import load_embedder, embed_triple, embed_text, dedupe
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æœ¬å°ˆæ¡ˆè‡ªè£½æ¨¡çµ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from .core.paths import (
    CKIP_ROOT,
    KG_EMB_PATH,
    KG_CSV_PATH,
    OUT_DIR,
    USER_INPUT_DIR,
    EXTRACT_PROMPT_PATH,
    JUDGE_PROMPT_PATH,
)
from .core.utils import safe_json_loads, clean_json_block
from .kg.loader import load_kg_vectors, load_kg_df
from .kg.search import search_by_triples
from .llm.gpt import GPTClient
from .llm.prompt_loader import load_prompt
from ..tools import data_utils as du
# éœ€ç”¨åˆ° qa.tools ç”Ÿæˆæ•˜è¿°å€å¡Š
from ..tools import kg_nl as knl

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åƒæ•¸è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SIM_TH: float = 0.80  # KG ç›¸ä¼¼åº¦é–€æª»
TOP_K: int = 100  # æ¯å€‹ä¸‰å…ƒçµ„å–å‰ TOP_K æ¢


def main() -> None:
    parser = argparse.ArgumentParser(description="Answerer pipeline: æŒ‡å®šå•é¡Œæª”æ¡ˆ <id>.txt")
    parser.add_argument("input_file", help="Path or filename of question file, e.g. '2024-...txt'")
    args = parser.parse_args()

    # è®€å–å•é¡Œæª”æ¡ˆ
    input_path = Path(args.input_file)
    if not input_path.is_file():
        candidate = Path(USER_INPUT_DIR) / args.input_file
        if candidate.is_file():
            input_path = candidate
    if not input_path.is_file():
        for p in Path(USER_INPUT_DIR).rglob(Path(args.input_file).name):
            input_path = p
            break
    if not input_path.is_file():
        sys.exit(f"âŒ ç„¡æ•ˆçš„è¼¸å…¥æª”æ¡ˆ: {args.input_file}")

    question = input_path.read_text(encoding="utf-8").strip()
    slug = input_path.stem
    print(f"ğŸ”¸ Question: {question}")

    # è³‡æºåˆå§‹åŒ–
    emb = load_embedder(CKIP_ROOT)
    kg_vecs, kg_vecs_norm = load_kg_vectors(KG_EMB_PATH)
    kg_df, hp_col, rp_col, tp_col = load_kg_df(KG_CSV_PATH)
    extract_prompt = load_prompt(EXTRACT_PROMPT_PATH)
    judge_prompt = load_prompt(JUDGE_PROMPT_PATH)
    gpt = GPTClient(
        api_key=os.getenv("GPT_API"),
        model_id=os.getenv("GPT_MODEL", "gpt-4o"),
        temperature=0.4,
        top_p=0.9,
        max_tokens=2048,
    )

    # 2. å‘¼å« GPT æŠ½å–ä¸‰å…ƒçµ„
    raw_resp = gpt.chat(extract_prompt, question)
    print("ğŸªµ GPT raw response:\n", raw_resp)

    # æ“·å– JSON block
    block = clean_json_block(raw_resp)
    # ç§»é™¤æ‰€æœ‰åå¼•è™Ÿï¼Œä¸¦å»æ‰å¯èƒ½çš„ "json" å‰ç¶´
    cleaned = re.sub(r'^\s*json\s*', '', block, flags=re.IGNORECASE)
    cleaned = cleaned.replace("`", "").strip()
    print("ğŸªµ Cleaned JSON block:\n", cleaned)

    try:
        data = safe_json_loads(cleaned)
    except Exception as e:
        print("[ERROR] ç„¡æ³•è§£æ JSONï¼Œcleaned å…§å®¹å¦‚ä¸‹ï¼š", cleaned)
        sys.exit("âŒ GPT å›å‚³çš„å…§å®¹ä¸æ˜¯åˆæ³• JSONï¼Œè«‹æª¢æŸ¥æ¨¡å‹è¼¸å‡ºèˆ‡ prompt è¨­å®š")

    if isinstance(data, dict) and "triples" in data:
        triples = [
            {"head": t["subject"], "relation": t["relation"], "tail": t["object"]}
            for t in data["triples"]
            if t.get("subject") and t.get("relation")
        ]
    else:
        triples = du.json_to_triples(data) or []
    print(f"ğŸª² Parsed triples count: {len(triples)}")
    if not triples:
        sys.exit("âŒ GPT æœªæŠ½å–åˆ°ä¸‰å…ƒçµ„")

    # 3. KG å‘é‡æª¢ç´¢
    raw_lines = search_by_triples(
        triples,
        embed_fn=lambda tp: embed_triple(emb, tp),
        kg_vecs_norm=kg_vecs_norm,
        top_k=TOP_K,
        sim_th=SIM_TH,
        kg_df=kg_df,
        hp_col=hp_col,
        rp_col=rp_col,
        tp_col=tp_col,
        build_block_fn=knl.build_block,
    )
    if not raw_lines:
        sys.exit("âš ï¸ KG ç„¡ä»»ä½•åŒ¹é…")

    # 4. èªæ„å»é‡
    final_lines = dedupe(
        raw_lines,
        embed_fn=lambda ln: embed_text(emb, ln),
        threshold=0.80,
    )

    # 5. è¼¸å‡ºè‡³æª”æ¡ˆ
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    kg_out = OUT_DIR / f"user_kg_{slug}.txt"
    judge_out = OUT_DIR / f"user_qa_judge_{slug}.txt"

    kg_out.write_text(
        "[ä½¿ç”¨è€…æå•]\n"
        f"{question}\n\n[çŸ¥è­˜æŸ¥è©¢çµæœ]\n"
        + "\n".join(final_lines)
        + "\n",
        encoding="utf-8",
    )

    # 6. GPT æœ€çµ‚åˆ¤æ–·
    judge_result = gpt.chat(judge_prompt, kg_out.read_text(encoding="utf-8-sig"))
    # ç§»é™¤æ‰€æœ‰åå¼•è™Ÿã€äº•è™Ÿèˆ‡æ˜Ÿè™Ÿ
    judge_result = (judge_result
                    .replace("`", "")
                    .replace("#", "")
                    .replace("*", "")
                    )
    judge_out.write_text(judge_result, encoding="utf-8-sig")

    print("âœ… finished; outputs saved under", OUT_DIR)
    print("   KG    â†’", kg_out.name)
    print("   JUDGE â†’", judge_out.name)


if __name__ == "__main__":
    main()
