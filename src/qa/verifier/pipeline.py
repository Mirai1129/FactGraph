#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°èäº‹å¯¦é©—è­‰ä¸»æµç¨‹ Pipeline

åŸ·è¡Œæ–¹å¼ï¼š
  - å…¨é‡ï¼špython -m src.qa.verifier.pipeline
  - å–®ç¯‡ï¼špython -m src.qa.verifier.pipeline <news_id>
"""

from __future__ import annotations

import argparse
import json
import re
import sys
import time
import gc
from pathlib import Path
from typing import List

import numpy as np
from tqdm import tqdm

from ..tools import data_utils as du
from ..tools import kg_nl as knl
from .core.paths import USER_INPUT_DIR, VEC_DIR, RES_DIR
from .core.config import LLM_ROUNDS
from .core.embeddings import embed_text, embed_triple
from .kg.search import cosine_search, kg_row_to_detail
from .llm.extract import extract_entities_relations
from .llm.judge import judge_news_kb
from .core.dedup import deduplicate


def _pull_triples(text: str) -> List[du.Triple]:
    """
    å¤šè¼ª LLM æŠ½å–ä¸‰å…ƒçµ„ä¸¦å»é‡åˆä½µã€‚
    å›å‚³åˆä½µå¾Œçš„ä¸‰å…ƒçµ„åˆ—è¡¨ã€‚
    """
    all_rounds: List[List[du.Triple]] = []
    last_error: Exception | None = None

    for i in range(LLM_ROUNDS):
        print(f'ğŸ”¸ GPT æŠ½å– round {i+1}')
        start = time.time()
        raw = extract_entities_relations(text)
        elapsed = time.time() - start
        print(f'  â†³ å®Œæˆï¼Œç”¨æ™‚ {elapsed:.1f}s')

        if not raw:
            print(f'[WARN] æŠ½å–å›å‚³ç‚ºç©ºï¼Œè·³é round {i+1}')
            continue

        try:
            triples = du.json_to_triples(json.loads(raw))
            all_rounds.append(triples)
        except Exception as e:
            last_error = e
            print(f'[WARN] JSON è§£æå¤±æ•—æ–¼ round {i+1}: {e}')

    if not all_rounds:
        if last_error:
            print(f'[ERROR] æ‰€æœ‰è¼ªæ¬¡çš†å¤±æ•—: {last_error}', file=sys.stderr)
        return []

    return du.merge_triples(*all_rounds)


def _process_single(news_id: str, text: str) -> None:
    """
    è™•ç†å–®ç¯‡æ–°èï¼š
      1. åµŒå…¥å…¨æ–‡
      2. LLM æŠ½å–ä¸‰å…ƒçµ„
      3. å‘é‡æª¢ç´¢ KG
      4. å»é‡èˆ‡ç·¨è™Ÿ
      5. è¼¸å‡ºçµæœèˆ‡äº‹å¯¦åˆ¤æ–·
    """
    RES_DIR.mkdir(parents=True, exist_ok=True)
    VEC_DIR.mkdir(parents=True, exist_ok=True)

    # å…¨æ–‡åµŒå…¥
    vec_path = VEC_DIR / f'{news_id}.npy'
    np.save(vec_path, embed_text(text))

    # ä¸‰å…ƒçµ„æŠ½å–
    triples = _pull_triples(text)
    if not triples:
        sys.exit('âŒ LLM æœªæŠ½å–åˆ°ä»»ä½•ä¸‰å…ƒçµ„ï¼Œæµç¨‹çµ‚æ­¢')

    # KG æ¯”å°
    raw_lines: List[str] = []
    for tp in tqdm(triples, desc='ğŸ” KG æ¯”å°'):
        q_vec = embed_triple(tp)
        for idx in cosine_search(tp, q_vec):
            tri, det = kg_row_to_detail(idx)
            block = knl.build_block([tri], {tuple(tri.values()): det})
            raw_lines.extend(block.splitlines())

    if not raw_lines:
        sys.exit('âš ï¸ ç„¡ KG å‘½ä¸­')

    # å»é‡èˆ‡é‡ç·¨è™Ÿ
    kept = deduplicate(raw_lines)
    final = [re.sub(r'^\d+\.', f'[{i}]', ln, count=1) for i, ln in enumerate(kept, 1)]

    # çµ„åˆè¼¸å‡º
    news_block = f"```\n[åŸå§‹æ–°è]\n{text}\n```"
    kb_block = '```\n[æ¯”å°çŸ¥è­˜]\n' + '\n'.join(final) + '\n```'

    # å¯«å…¥æª”æ¡ˆ
    kg_file = RES_DIR / f'news_kg_{news_id}.txt'
    judge_file = RES_DIR / f'judge_result_{news_id}.txt'
    kg_file.write_text(f'{news_block}\n{kb_block}', encoding='utf-8')

    # äº‹å¯¦åˆ¤æ–·
    judged = judge_news_kb(f'{news_block}\n{kb_block}')
    judge_file.write_text(judged, encoding='utf-8')

    print(f'âœ… è¼¸å‡ºï¼š{kg_file.name}, {judge_file.name}')


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser('FactGraph Verifier Pipeline')
    p.add_argument(
        'news_id',
        nargs='?',
        help='æ–°èæª”åï¼ˆä¸å« .txtï¼‰ï¼Œç•™ç©ºå‰‡æ‰¹æ¬¡æ‰€æœ‰'
    )
    return p.parse_args()


def main() -> None:
    args = _parse_args()

    if args.news_id:
        input_path = USER_INPUT_DIR / f'{args.news_id}.txt'
        if not input_path.is_file():
            sys.exit(f'âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{input_path}')
        text = input_path.read_text(encoding='utf-8-sig').strip()
        _process_single(args.news_id, text)
    else:
        processed = {
            p.stem.removeprefix('news_kg_')
            for p in RES_DIR.glob('news_kg_*.txt')
        }
        for path in sorted(USER_INPUT_DIR.glob('*.txt')):
            nid = path.stem
            if nid in processed:
                continue
            text = path.read_text(encoding='utf-8-sig').strip()
            _process_single(nid, text)

    gc.collect()


if __name__ == '__main__':
    print(f'âš™ï¸ USER_INPUT_DIR: {USER_INPUT_DIR.resolve()} (exists: {USER_INPUT_DIR.is_dir()})')
    print(f'âš™ï¸ RES_DIR:        {RES_DIR.resolve()} (exists: {RES_DIR.is_dir()})')
    files = list(USER_INPUT_DIR.glob("*.txt"))
    print(f'ğŸ” æ‰¾åˆ° {len(files)} å€‹è¼¸å…¥æª”æ¡ˆï¼š{[p.name for p in files]}')

    main()
