"""
answerer â”€ å•ç­”ä¸»æµç¨‹ï¼ˆOrchestratorï¼‰

è·è²¬åªåšã€Œæµç¨‹å”èª¿ã€ï¼Œä¸è™•ç†ç¹é›œæ¥­å‹™é‚è¼¯ï¼š
0. python -m src.qa.answerer.pipeline <id.txt>
1. è®€å–ä½¿ç”¨è€…å•é¡Œï¼ˆæª”æ¡ˆæˆ– stdinï¼‰ä¸¦å–å¾— slug
2. å‘¼å« GPT æŠ½å–ä¸‰å…ƒçµ„
3. ä»¥å‘é‡æœå°‹ KG ç›¸é—œæ•˜è¿°
4. å»é‡ï¼ˆç›¸ä¼¼åƒ…ä¿ç•™æœ€é•·æ¢ç›®ï¼‰
5. å‘¼å« GPT è©•ä¼°æœ€çµ‚çµæœ
6. ä¾è¼¸å…¥æª”åå‹•æ…‹è¼¸å‡º user_kg_*.txt èˆ‡ user_qa_judge_*.txt
"""

from __future__ import annotations

import os
import sys
import argparse
from pathlib import Path
from typing import List, Dict

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æœ¬å°ˆæ¡ˆè‡ªè£½æ¨¡çµ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from .core.paths import (
    CKIP_ROOT,
    KG_EMB_PATH,
    KG_CSV_PATH,
    OUT_DIR,
    USER_INPUT_DIR,
    EXTRACT_PROMPT_PATH,
    JUDGE_PROMPT_PATH,
)
from .core.embedding import load_embedder, embed_triple, embed_text, dedupe
from .core.utils import safe_json_loads, clean_json_block
from .kg.loader import load_kg_vectors, load_kg_df
from .kg.search import search_by_triples
from .llm.gpt import GPTClient
from .llm.prompt_loader import load_prompt

# éœ€ç”¨åˆ° qa.tools ç”Ÿæˆæ•˜è¿°å€å¡Š
from ..tools import kg_nl as knl
from ..tools import data_utils as du

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åƒæ•¸è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SIM_TH: float = 0.80          # KG ç›¸ä¼¼åº¦é–€æª»
TOP_K: int = 100              # æ¯å€‹ä¸‰å…ƒçµ„å–å‰ TOP_K æ¢

# ---------------------------------------------------------------------------
# ä¸»æµç¨‹
# ---------------------------------------------------------------------------

def main() -> None:
    """æ•´å€‹å•ç­”ç®¡ç·šçš„å…¥å£å‡½å¼ï¼Œä¾å‘½ä»¤åˆ—åƒæ•¸æŒ‡å®šè¼¸å…¥æª”æ¡ˆ <id>.txtã€‚"""

    # ========== 0. è§£ææŒ‡ä»¤åƒæ•¸ ==========
    parser = argparse.ArgumentParser(description="Answerer pipeline: æŒ‡å®šå•é¡Œæª”æ¡ˆ <id>.txt æˆ–ç›¸å°è·¯å¾‘")
    parser.add_argument(
        "input_file", help="Path or filename of question file, e.g. '2024-...txt'"
    )
    args = parser.parse_args()

    # å˜—è©¦ç›´æ¥è®€å–
    input_path = Path(args.input_file)
    # è‹¥ä¸å­˜åœ¨ï¼Œæª¢æŸ¥ USER_INPUT_DIR ä¸‹ç›´å±¬
    if not input_path.is_file():
        candidate = Path(USER_INPUT_DIR) / args.input_file
        if candidate.is_file():
            input_path = candidate
    # è‹¥ä»ä¸å­˜åœ¨ï¼Œéè¿´æœå°‹ USER_INPUT_DIR
    if not input_path.is_file():
        for path in Path(USER_INPUT_DIR).rglob(Path(args.input_file).name):
            input_path = path
            break
    if not input_path.is_file():
        sys.exit(f"âŒ ç„¡æ•ˆçš„è¼¸å…¥æª”æ¡ˆ: {args.input_file}")

    # å–å¾—å•é¡Œå…§å®¹èˆ‡ slug
    question = input_path.read_text(encoding="utf-8").strip()
    slug = input_path.stem
    print(f"ğŸ”¸ Question: {question}")

    # ========== 1. è³‡æºåˆå§‹åŒ– ==========
    emb = load_embedder(CKIP_ROOT)
    kg_vecs, kg_vecs_norm = load_kg_vectors(KG_EMB_PATH)
    kg_df, hp_col, rp_col, tp_col = load_kg_df(KG_CSV_PATH)
    extract_prompt: str = load_prompt(EXTRACT_PROMPT_PATH)
    judge_prompt: str = load_prompt(JUDGE_PROMPT_PATH)
    gpt = GPTClient(
        api_key=os.getenv("GPT_API"),
        model_id=os.getenv("GPT_MODEL", "gpt-4o"),
        temperature=0.4,
        top_p=0.9,
        max_tokens=2048,
    )

    # ========== 2. å‘¼å« GPT æŠ½å–ä¸‰å…ƒçµ„ ==========
    raw_resp: str = gpt.chat(extract_prompt, question)
    print("ğŸªµ GPT raw response:\n", raw_resp)
    cleaned = clean_json_block(raw_resp)
    data = safe_json_loads(cleaned)
    if isinstance(data, dict) and "triples" in data:
        triples: List[Dict[str, str]] = [
            {"head": t.get("subject"), "relation": t.get("relation"), "tail": t.get("object")}  # type: ignore
            for t in data["triples"]
            if t.get("subject") and t.get("relation")
        ]
    else:
        triples = du.json_to_triples(data) or []  # type: ignore
    print(f"ğŸª² Parsed triples count: {len(triples)}")
    if not triples:
        sys.exit("âŒ GPT æœªæŠ½å–åˆ°ä¸‰å…ƒçµ„")

    # ========== 3. KG å‘é‡æª¢ç´¢ ==========
    raw_lines = search_by_triples(
        triples,
        embed_fn=lambda tp: embed_triple(emb, tp),
        kg_vecs_norm=kg_vecs_norm,
        top_k=TOP_K,
        sim_th=SIM_TH,
        kg_df=kg_df,
        hp_col=hp_col,
        rp_col=rp_col,
        tp_col=tp_col,
        build_block_fn=knl.build_block,
    )
    if not raw_lines:
        sys.exit("âš ï¸ KG ç„¡ä»»ä½•åŒ¹é…")

    # ========== 4. èªæ„å»é‡ï¼ˆç›¸ä¼¼ä¿ç•™æœ€é•·ï¼‰ ==========
    final_lines = dedupe(
        raw_lines,
        embed_fn=lambda ln: embed_text(emb, ln),
        threshold=0.80,
    )

    # ========== 5. ä¾ slug å‹•æ…‹è¼¸å‡º ==========
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    kg_out: Path = OUT_DIR / f"user_kg_{slug}.txt"
    judge_out: Path = OUT_DIR / f"user_qa_judge_{slug}.txt"

    kg_out.write_text(
        "```\n[ä½¿ç”¨è€…æå•]\n"
        f"{question}\n```\n```\n[çŸ¥è­˜æŸ¥è©¢çµæœ]\n"
        + "\n".join(final_lines)
        + "\n```",
        encoding="utf-8",
    )

    # ========== 6. GPT æœ€çµ‚åˆ¤æ–· ==========
    judge_result = gpt.chat(judge_prompt, kg_out.read_text(encoding="utf-8-sig"))
    judge_out.write_text(judge_result, encoding="utf-8-sig")

    print("âœ… finished; outputs saved under", OUT_DIR)
    print("   KG    â†’", kg_out.name)
    print("   JUDGE â†’", judge_out.name)


# ---------------------------------------------------------------------------
# CLI åŸ·è¡Œ
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    main()
